{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9170481,"sourceType":"datasetVersion","datasetId":5541561}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install necessary libraries\n!pip install torch transformers pandas sklearn flask docker","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:20:46.244325Z","iopub.execute_input":"2024-08-14T09:20:46.244909Z","iopub.status.idle":"2024-08-14T09:20:49.277232Z","shell.execute_reply.started":"2024-08-14T09:20:46.244880Z","shell.execute_reply":"2024-08-14T09:20:49.276038Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nCollecting sklearn\n  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 1: Data Loading and Preprocessing\n\n1. **Load the Data**:\n   - Load the dataset from a JSON file into a Pandas DataFrame.\n\n2. **Filter Active Entries**:\n   - Keep only active entries (`status == 'A'`).\n\n3. **Encode Labels**:\n   - Use `LabelEncoder` to convert class IDs into numeric labels.\n\n4. **Tokenize Descriptions**:\n   - Tokenize the `description` column using BERT's tokenizer, limiting to 128 tokens.\n\n5. **Split the Dataset**:\n   - Split the data into training and validation sets (80/20 split).\n","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer\n\n\nwith open('/kaggle/input/idmanual/idmanual.json', 'r') as file:\n    data = json.load(file)\n\n\ndf = pd.DataFrame(data)\n\n\ndf = df[df['status'] == 'A']\n\n\nlabel_encoder = LabelEncoder()\ndf['class_id_encoded'] = label_encoder.fit_transform(df['class_id'])\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ndf['input_ids'] = df['description'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128, truncation=True))\n\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:20:52.200365Z","iopub.execute_input":"2024-08-14T09:20:52.201389Z","iopub.status.idle":"2024-08-14T09:21:16.379505Z","shell.execute_reply.started":"2024-08-14T09:20:52.201353Z","shell.execute_reply":"2024-08-14T09:21:16.378410Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"293f6ad5021b4d8fadac32684daccd81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69ec34caac0e496f9f021adee0d77ec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e18759860f7545f0a4423d7d1b03ac5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c72d3f57c7e47fd8de1a5f73b68681f"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 1: Model Definition and Initialization\n\n1. **Importing Libraries**:\n   - We import PyTorch and the pre-trained BERT model from the `transformers` library.\n\n2. **Defining the Model**:\n   - We create a `TrademarkClassifier` class using BERT as the base model.\n   - The model includes a dropout layer to prevent overfitting and a linear layer to output predictions for the number of trademark classes.\n\n3. **Setting Up the Model**:\n   - We determine the number of unique class labels (`num_labels`) and use this to define the output size of the model.\n   - We initialize the model with the correct number of labels.\n\n4. **Moving the Model to GPU**:\n   - The model is moved to GPU (if available) for faster training and inference.\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\nclass TrademarkClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super(TrademarkClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.linear = nn.Linear(768, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        dropout_output = self.dropout(pooled_output)\n        logits = self.linear(dropout_output)\n        return logits\n\nnum_labels = len(df['class_id_encoded'].unique())\nmodel = TrademarkClassifier(num_labels)\n\n# Move model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:21:21.521382Z","iopub.execute_input":"2024-08-14T09:21:21.521921Z","iopub.status.idle":"2024-08-14T09:21:25.719409Z","shell.execute_reply.started":"2024-08-14T09:21:21.521891Z","shell.execute_reply":"2024-08-14T09:21:25.718505Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac59baadedc84728874d55711479c9c8"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrademarkClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=47, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Creating DataLoaders\n\n1. **Import Required Modules**:\n   - We import `DataLoader` and `TensorDataset` from PyTorch, and `pad_sequence` for handling varying input lengths.\n\n2. **Function to Create DataLoader**:\n   - **Input IDs**: Convert tokenized input descriptions into tensors.\n   - **Padding**: Pad the input sequences so they all have the same length.\n   - **Labels**: Convert the encoded class labels into tensors.\n   - **TensorDataset**: Combine the input IDs and labels into a dataset.\n   - **DataLoader**: Create a DataLoader to efficiently manage batches during training.\n\n3. **Initialize DataLoaders**:\n   - We create `train_loader` and `val_loader` using the training and validation data.\n","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef create_data_loader(df, batch_size=16):\n    \n    input_ids = [torch.tensor(ids) for ids in df['input_ids'].tolist()]\n    \n    \n    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n    \n    \n    labels = torch.tensor(df['class_id_encoded'].values)\n    \n   \n    dataset = TensorDataset(input_ids_padded, labels)\n    \n    \n    return DataLoader(dataset, batch_size=batch_size)\n\ntrain_loader = create_data_loader(train_df)\nval_loader = create_data_loader(val_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:21:30.699116Z","iopub.execute_input":"2024-08-14T09:21:30.699895Z","iopub.status.idle":"2024-08-14T09:21:31.432025Z","shell.execute_reply.started":"2024-08-14T09:21:30.699863Z","shell.execute_reply":"2024-08-14T09:21:31.430907Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Step: Training and Evaluating the Model\n\n1. **Import Necessary Modules**:\n   - Import `Adam` for optimization and `accuracy_score` for evaluating model performance.\n\n2. **Training Function (`train_model`)**:\n   - **Optimizer and Loss**: Initialize the Adam optimizer and CrossEntropyLoss.\n   - **Training Loop**:\n     - Iterate over epochs and batches.\n     - Move input data and labels to GPU.\n     - Perform a forward pass, compute loss, backpropagate, and update weights.\n     - Track and print the average loss per epoch.\n\n3. **Evaluation Function (`evaluate_model`)**:\n   - **Evaluation Mode**: Set the model to evaluation mode.\n   - **Prediction and Accuracy**:\n     - Move data to GPU.\n     - Make predictions and compute accuracy using `accuracy_score`.\n     - Print the validation accuracy after each epoch.\n\n4. **Running the Training**:\n   - Call `train_model` to start training the model for 3 epochs and evaluate it after each epoch.\n","metadata":{}},{"cell_type":"code","source":"from torch.optim import Adam\nfrom sklearn.metrics import accuracy_score\n\n\ndef train_model(model, train_loader, val_loader, epochs=3):\n    optimizer = Adam(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids, labels = batch\n            \n            # Move tensors to GPU\n            input_ids = input_ids.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids=input_ids, attention_mask=(input_ids > 0))\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n        \n        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n        evaluate_model(model, val_loader)\n\n\ndef evaluate_model(model, val_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids, labels = batch\n            \n            \n            input_ids = input_ids.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=(input_ids > 0))\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f'Validation Accuracy: {accuracy}')\n\n# Train the model\ntrain_model(model, train_loader, val_loader, epochs=3)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T07:18:15.053686Z","iopub.execute_input":"2024-08-14T07:18:15.054040Z","iopub.status.idle":"2024-08-14T07:53:29.842578Z","shell.execute_reply.started":"2024-08-14T07:18:15.054013Z","shell.execute_reply":"2024-08-14T07:53:29.841658Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.2562365964741695\nValidation Accuracy: 0.8498383185125303\nEpoch 2, Loss: 0.339635901520497\nValidation Accuracy: 0.8769199676637025\nEpoch 3, Loss: 0.1392535182100434\nValidation Accuracy: 0.8966248989490704\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_class(description, model, tokenizer, label_encoder, max_length=128, device='cpu'):\n    \n    \n    input_ids = torch.tensor([tokenizer.encode(description, add_special_tokens=True, max_length=max_length, truncation=True)])\n    \n    \n    input_ids = input_ids.to(device)\n    \n    \n    model.eval()\n    \n    with torch.no_grad():\n        \n        outputs = model(input_ids=input_ids, attention_mask=(input_ids > 0))\n        prediction = torch.argmax(outputs, dim=1).item()\n    \n    \n    class_id = label_encoder.inverse_transform([prediction])[0]\n    return class_id\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:21:43.144227Z","iopub.execute_input":"2024-08-14T09:21:43.144619Z","iopub.status.idle":"2024-08-14T09:21:43.152870Z","shell.execute_reply.started":"2024-08-14T09:21:43.144589Z","shell.execute_reply":"2024-08-14T09:21:43.151817Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\ntest_description = \"DVD recorders\"\n\n# Predict the class ID\npredicted_class = predict_class(test_description, model, tokenizer, label_encoder, device=device)\n\nprint(f\"Predicted Class ID: {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:21:46.123780Z","iopub.execute_input":"2024-08-14T09:21:46.124426Z","iopub.status.idle":"2024-08-14T09:21:47.229027Z","shell.execute_reply.started":"2024-08-14T09:21:46.124396Z","shell.execute_reply":"2024-08-14T09:21:47.228007Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Predicted Class ID: 040\n","output_type":"stream"}]},{"cell_type":"code","source":"# List of new descriptions\ndescriptions = [\n    \"Scientific apparatus for measuring DNA concentration\",\n    \"Wireless adapters for computers\",\n    \"Notebook computer carrying cases\",\n    \"DVD recorders\"\n]\n\n# Predict and print class IDs for each description\nfor desc in descriptions:\n    predicted_class = predict_class(desc, model, tokenizer, label_encoder, device=device)\n    print(f\"Description: {desc}\")\n    print(f\"Predicted Class ID: {predicted_class}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:22:08.797358Z","iopub.execute_input":"2024-08-14T09:22:08.798217Z","iopub.status.idle":"2024-08-14T09:22:08.848872Z","shell.execute_reply.started":"2024-08-14T09:22:08.798182Z","shell.execute_reply":"2024-08-14T09:22:08.847860Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Description: Scientific apparatus for measuring DNA concentration\nPredicted Class ID: 010\n\nDescription: Wireless adapters for computers\nPredicted Class ID: 010\n\nDescription: Notebook computer carrying cases\nPredicted Class ID: 010\n\nDescription: DVD recorders\nPredicted Class ID: 040\n\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndifferent_description = \"Cosmetic applicators for applying makeup\"\n\n# Predict the class ID\npredicted_class = predict_class(different_description, model, tokenizer, label_encoder, device=device)\n\nprint(f\"Predicted Class ID: {predicted_class}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:22:22.042228Z","iopub.execute_input":"2024-08-14T09:22:22.042616Z","iopub.status.idle":"2024-08-14T09:22:22.085041Z","shell.execute_reply.started":"2024-08-14T09:22:22.042588Z","shell.execute_reply":"2024-08-14T09:22:22.084037Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Predicted Class ID: 010\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndifferent_description = \"Cheese Burgers\"\n\n# Predict the class ID\npredicted_class = predict_class(different_description, model, tokenizer, label_encoder, device=device)\n\nprint(f\"Predicted Class ID: {predicted_class}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:22:34.387708Z","iopub.execute_input":"2024-08-14T09:22:34.388413Z","iopub.status.idle":"2024-08-14T09:22:34.407384Z","shell.execute_reply.started":"2024-08-14T09:22:34.388379Z","shell.execute_reply":"2024-08-14T09:22:34.406301Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Predicted Class ID: 010\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport pickle\nimport torch.nn as nn\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer, BertModel\n\n\nlabel_encoder = LabelEncoder()\ndf['class_id_encoded'] = label_encoder.fit_transform(df['class_id'])\n\n\nnum_labels = len(label_encoder.classes_)\n\n\nclass TrademarkClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super(TrademarkClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.linear = nn.Linear(768, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        dropout_output = self.dropout(pooled_output)\n        logits = self.linear(dropout_output)\n        return logits\n\n\nmodel = TrademarkClassifier(num_labels)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\nmodel_save_path = \"trademark_classifier_model.pt\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n\n\nwith open(\"label_encoder.pkl\", \"wb\") as f:\n    pickle.dump(label_encoder, f)\nprint(\"Label Encoder saved to label_encoder.pkl\")\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\nprint(\"Tokenizer saved to tokenizer.pkl\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:26:11.104747Z","iopub.execute_input":"2024-08-14T09:26:11.105352Z","iopub.status.idle":"2024-08-14T09:26:12.687129Z","shell.execute_reply.started":"2024-08-14T09:26:11.105321Z","shell.execute_reply":"2024-08-14T09:26:12.686139Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model saved to trademark_classifier_model.pt\nLabel Encoder saved to label_encoder.pkl\nTokenizer saved to tokenizer.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport pickle\nfrom flask import Flask, request, jsonify\n\n\nclass TrademarkClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super(TrademarkClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.linear = nn.Linear(768, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[1]\n        dropout_output = self.dropout(pooled_output)\n        logits = self.linear(dropout_output)\n        return logits\n\n\nwith open(\"label_encoder.pkl\", \"rb\") as f:\n    label_encoder = pickle.load(f)\n\n\nwith open(\"tokenizer.pkl\", \"rb\") as f:\n    tokenizer = pickle.load(f)\n\n\nnum_labels = len(label_encoder.classes_)\n\n\nmodel = TrademarkClassifier(num_labels)\nmodel.load_state_dict(torch.load(\"trademark_classifier_model.pt\"))\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T09:27:56.723659Z","iopub.execute_input":"2024-08-14T09:27:56.724582Z","iopub.status.idle":"2024-08-14T09:27:57.852571Z","shell.execute_reply.started":"2024-08-14T09:27:56.724540Z","shell.execute_reply":"2024-08-14T09:27:57.851573Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrademarkClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=47, bias=True)\n)"},"metadata":{}}]}]}